"""
ë…¼ë€ íƒì§€ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ: 1í•™ê¸° vs 2í•™ê¸°
- 1í•™ê¸°: ["controversial", "non-controversial"] ë¼ë²¨
- 2í•™ê¸°: ["direct accusation: fraud/scam/undisclosed promotion", "general comment"] ë¼ë²¨
- DeepL APIë¡œ í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­ í›„ ë¶„ì„
"""
from __future__ import annotations

import re
import time
import asyncio
import httpx
from typing import List, Dict, Tuple
from dataclasses import dataclass

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# í…ŒìŠ¤íŠ¸ ë°ì´í„° import
try:
    from scripts.test_comments_100 import TEST_COMMENTS, CATEGORY_INFO, get_stats
except ModuleNotFoundError:
    from test_comments_100 import TEST_COMMENTS, CATEGORY_INFO, get_stats

# DeepL API í‚¤ import
try:
    from src.config import DEEPL_API_KEY
except ModuleNotFoundError:
    try:
        from config import DEEPL_API_KEY
    except ModuleNotFoundError:
        import os
        from dotenv import load_dotenv
        load_dotenv()
        DEEPL_API_KEY = os.getenv("DEEPL_API_KEY")

# ============================================================
# ëª¨ë¸ ì„¤ì •
# ============================================================
_MNAME = "facebook/bart-large-mnli"

# 1í•™ê¸° ì„¤ì •
V1_LABELS = ["controversial", "non-controversial"]
V1_HYPO = "This text is {}."
V1_THRESHOLD = 0.7
V1_RATIO_THRESHOLD = 0.10

# 2í•™ê¸° ì„¤ì •
V2_LABELS = [
    "direct accusation: this is fraud, scam, or undisclosed paid promotion",
    "general comment, opinion, or complaint"
]
V2_HYPO = "This comment is: {}."
V2_THRESHOLD = 0.35
V2_RATIO_THRESHOLD = 0.20
V2_CONTROVERSY_LABELS = V2_LABELS[:1]

# ê³µìœ  ëª¨ë¸
_clf = None


def _get_classifier():
    """ëª¨ë¸ ì§€ì—° ë¡œë”©"""
    global _clf
    if _clf is None:
        print("[INFO] BART ëª¨ë¸ ë¡œë”© ì¤‘...")
        tok = AutoTokenizer.from_pretrained(_MNAME)
        model = AutoModelForSequenceClassification.from_pretrained(
            _MNAME, use_safetensors=True
        )
        _clf = pipeline(
            task="zero-shot-classification",
            model=model,
            tokenizer=tok,
            device=0 if torch.cuda.is_available() else -1,
        )
        print("[SUCCESS] ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    return _clf


# ============================================================
# DeepL ë²ˆì—­ í•¨ìˆ˜
# ============================================================
def _is_english(text: str) -> bool:
    """ì˜ì–´ì¸ì§€ í™•ì¸"""
    cleaned = re.sub(r"[^\w\s.,!?\'\"-]", "", text or "")
    return bool(re.fullmatch(r"[A-Za-z0-9\s\.,;:'\"!?()\[\]{}@#$%^&*_\-=+/<>|~]+", cleaned))


async def translate_to_english(texts: List[str]) -> List[str]:
    """DeepL APIë¡œ í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­ (ë¹„ë™ê¸°)"""
    if not DEEPL_API_KEY:
        print("[WARN] DEEPL_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. ì›ë¬¸ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
        return texts
    
    # DeepL API URL (ìœ ë£Œ ë²„ì „)
    url = "https://api.deepl.com/v2/translate"
    translated: List[str] = []
    
    async with httpx.AsyncClient(timeout=20.0) as client:
        for text in texts:
            # ì´ë¯¸ ì˜ì–´ë©´ ë²ˆì—­ ìŠ¤í‚µ
            if _is_english(text):
                translated.append(text)
                continue
            
            try:
                response = await client.post(
                    url,
                    data={
                        "auth_key": DEEPL_API_KEY,
                        "text": text,
                        "target_lang": "EN"
                    }
                )
                response.raise_for_status()
                result = response.json()
                translated.append(result["translations"][0]["text"])
            except Exception as e:
                print(f"[WARN] ë²ˆì—­ ì‹¤íŒ¨: {text[:20]}... â†’ ì›ë¬¸ ì‚¬ìš©")
                translated.append(text)
    
    return translated


def translate_sync(texts: List[str]) -> List[str]:
    """ë™ê¸° ë²„ì „ ë²ˆì—­ í•¨ìˆ˜"""
    return asyncio.run(translate_to_english(texts))


# ============================================================
# ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤
# ============================================================
@dataclass
class CommentResult:
    text: str
    translated: str
    v1_score: float
    v2_score: float
    v1_flagged: bool
    v2_flagged: bool
    expected: bool


@dataclass
class ComparisonSummary:
    total_comments: int
    v1_flagged_count: int
    v2_flagged_count: int
    v1_accuracy: float
    v2_accuracy: float
    v1_precision: float
    v2_precision: float
    v1_recall: float
    v2_recall: float
    v1_f1: float
    v2_f1: float
    v1_time_ms: float
    v2_time_ms: float


# ============================================================
# 1í•™ê¸° ë²„ì „ (V1)
# ============================================================
def v1_controversy_scores(texts: List[str]) -> Tuple[List[float], float]:
    """1í•™ê¸° ë°©ì‹: controversial vs non-controversial"""
    clf = _get_classifier()
    
    start = time.time()
    outputs = clf(
        texts,
        candidate_labels=V1_LABELS,
        hypothesis_template=V1_HYPO,
        batch_size=16,
        multi_label=False,
    )
    elapsed_ms = (time.time() - start) * 1000
    
    scores = []
    for out in outputs:
        lbls = out.get("labels", [])
        scrs = out.get("scores", [])
        score = 0.0
        for lbl, sc in zip(lbls, scrs):
            if lbl == "controversial":
                score = float(sc)
                break
        scores.append(score)
    
    return scores, elapsed_ms


# ============================================================
# 2í•™ê¸° ë²„ì „ (V2)
# ============================================================
def v2_controversy_scores(texts: List[str]) -> Tuple[List[float], float]:
    """2í•™ê¸° ë°©ì‹: êµ¬ì²´ì  ì‚¬ê¸°/ë’·ê´‘ê³  ë ˆì´ë¸”"""
    clf = _get_classifier()
    
    start = time.time()
    outputs = clf(
        texts,
        candidate_labels=V2_LABELS,
        hypothesis_template=V2_HYPO,
        batch_size=16,
        multi_label=False,
    )
    elapsed_ms = (time.time() - start) * 1000
    
    scores = []
    for out in outputs:
        lbls = out.get("labels", [])
        scrs = out.get("scores", [])
        
        top_label = lbls[0] if lbls else ""
        top_score = scrs[0] if scrs else 0.0
        
        if top_label in V2_CONTROVERSY_LABELS:
            controversy_score = float(top_score)
        else:
            controversy_score = 0.0
        
        scores.append(controversy_score)
    
    return scores, elapsed_ms


# ============================================================
# ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
# ============================================================
def calculate_metrics(predictions: List[bool], ground_truth: List[bool]) -> Dict[str, float]:
    """ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ê³„ì‚°"""
    tp = sum(1 for p, g in zip(predictions, ground_truth) if p and g)
    fp = sum(1 for p, g in zip(predictions, ground_truth) if p and not g)
    fn = sum(1 for p, g in zip(predictions, ground_truth) if not p and g)
    tn = sum(1 for p, g in zip(predictions, ground_truth) if not p and not g)
    
    accuracy = (tp + tn) / len(predictions) if predictions else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "tp": tp,
        "fp": fp,
        "fn": fn,
        "tn": tn
    }


# ============================================================
# ë©”ì¸ ë¹„êµ í•¨ìˆ˜
# ============================================================
def compare_versions(test_data: List[Tuple[str, bool]]) -> Tuple[List[CommentResult], ComparisonSummary, Dict, Dict]:
    """ë‘ ë²„ì „ ë¹„êµ ì‹¤í–‰ (ë²ˆì—­ í¬í•¨)"""
    texts = [t[0] for t in test_data]
    ground_truth = [t[1] for t in test_data]
    
    print(f"\n{'='*70}")
    print(f"[ë¹„êµ ì‹œì‘] ì´ {len(texts)}ê°œ ëŒ“ê¸€ ë¶„ì„")
    print(f"{'='*70}\n")
    
    # ============================================================
    # 1ë‹¨ê³„: DeepL ë²ˆì—­
    # ============================================================
    print("[ë²ˆì—­] DeepL APIë¡œ í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­ ì¤‘...")
    translate_start = time.time()
    translated_texts = translate_sync(texts)
    translate_time = (time.time() - translate_start) * 1000
    print(f"       ì™„ë£Œ! ({translate_time:.1f}ms, {len(translated_texts)}ê°œ ë²ˆì—­)")
    
    # ë²ˆì—­ ìƒ˜í”Œ ì¶œë ¥
    print("\n[ë²ˆì—­ ìƒ˜í”Œ]")
    for i in [0, 40, 55]:  # ê¸ì •, ë…¼ë€, ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ì—ì„œ ìƒ˜í”Œ
        if i < len(texts):
            print(f"  ì›ë¬¸: {texts[i][:30]}...")
            print(f"  ë²ˆì—­: {translated_texts[i][:50]}...")
            print()
    
    # ============================================================
    # 2ë‹¨ê³„: V1 ì‹¤í–‰ (ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©)
    # ============================================================
    print("[V1] 1í•™ê¸° ë²„ì „ ì‹¤í–‰ ì¤‘... (controversial/non-controversial)")
    v1_scores, v1_time = v1_controversy_scores(translated_texts)
    v1_flagged = [s >= V1_THRESHOLD for s in v1_scores]
    print(f"     ì™„ë£Œ! ({v1_time:.1f}ms)")
    
    # ============================================================
    # 3ë‹¨ê³„: V2 ì‹¤í–‰ (ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©)
    # ============================================================
    print("[V2] 2í•™ê¸° ë²„ì „ ì‹¤í–‰ ì¤‘... (fraud/scam/promotion)")
    v2_scores, v2_time = v2_controversy_scores(translated_texts)
    v2_flagged = [s >= V2_THRESHOLD for s in v2_scores]
    print(f"     ì™„ë£Œ! ({v2_time:.1f}ms)")
    
    # ê°œë³„ ê²°ê³¼ ìƒì„±
    results = []
    for i, (text, expected) in enumerate(test_data):
        results.append(CommentResult(
            text=text,
            translated=translated_texts[i],
            v1_score=v1_scores[i],
            v2_score=v2_scores[i],
            v1_flagged=v1_flagged[i],
            v2_flagged=v2_flagged[i],
            expected=expected
        ))
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    v1_metrics = calculate_metrics(v1_flagged, ground_truth)
    v2_metrics = calculate_metrics(v2_flagged, ground_truth)
    
    summary = ComparisonSummary(
        total_comments=len(texts),
        v1_flagged_count=sum(v1_flagged),
        v2_flagged_count=sum(v2_flagged),
        v1_accuracy=v1_metrics["accuracy"],
        v2_accuracy=v2_metrics["accuracy"],
        v1_precision=v1_metrics["precision"],
        v2_precision=v2_metrics["precision"],
        v1_recall=v1_metrics["recall"],
        v2_recall=v2_metrics["recall"],
        v1_f1=v1_metrics["f1"],
        v2_f1=v2_metrics["f1"],
        v1_time_ms=v1_time,
        v2_time_ms=v2_time
    )
    
    return results, summary, v1_metrics, v2_metrics


def print_results(results: List[CommentResult], summary: ComparisonSummary, 
                  v1_metrics: Dict, v2_metrics: Dict):
    """ê²°ê³¼ ì¶œë ¥"""
    
    print(f"\n{'='*70}")
    print("ğŸ“Š ê°œë³„ ëŒ“ê¸€ ë¶„ì„ ê²°ê³¼")
    print(f"{'='*70}")
    print(f"{'#':<3} {'ì •ë‹µ':<6} {'V1ì ìˆ˜':<8} {'V1íŒì •':<8} {'V2ì ìˆ˜':<8} {'V2íŒì •':<8} ëŒ“ê¸€")
    print("-" * 70)
    
    for i, r in enumerate(results, 1):
        expected_str = "ğŸ”´ë…¼ë€" if r.expected else "ğŸŸ¢ì •ìƒ"
        v1_flag_str = "âš ï¸íƒì§€" if r.v1_flagged else "âœ…ì •ìƒ"
        v2_flag_str = "âš ï¸íƒì§€" if r.v2_flagged else "âœ…ì •ìƒ"
        
        v1_correct = "âœ“" if r.v1_flagged == r.expected else "âœ—"
        v2_correct = "âœ“" if r.v2_flagged == r.expected else "âœ—"
        
        text_preview = r.text[:25] + "..." if len(r.text) > 25 else r.text
        
        print(f"{i:<3} {expected_str:<6} {r.v1_score:<8.3f} {v1_flag_str}{v1_correct:<3} "
              f"{r.v2_score:<8.3f} {v2_flag_str}{v2_correct:<3} {text_preview}")
    
    # ìš”ì•½ í†µê³„
    print(f"\n{'='*70}")
    print("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ìš”ì•½")
    print(f"{'='*70}")
    print(f"{'ë©”íŠ¸ë¦­':<20} {'1í•™ê¸° (V1)':<20} {'2í•™ê¸° (V2)':<20} {'ì°¨ì´':<15}")
    print("-" * 70)
    
    metrics = [
        ("ì •í™•ë„ (Accuracy)", summary.v1_accuracy, summary.v2_accuracy),
        ("ì •ë°€ë„ (Precision)", summary.v1_precision, summary.v2_precision),
        ("ì¬í˜„ìœ¨ (Recall)", summary.v1_recall, summary.v2_recall),
        ("F1 Score", summary.v1_f1, summary.v2_f1),
    ]
    
    for name, v1, v2 in metrics:
        diff = v2 - v1
        diff_str = f"+{diff*100:.1f}%p" if diff >= 0 else f"{diff*100:.1f}%p"
        better = "â¬†ï¸" if diff > 0 else ("â¬‡ï¸" if diff < 0 else "â¡ï¸")
        print(f"{name:<20} {v1*100:>6.1f}%{'':<12} {v2*100:>6.1f}%{'':<12} {diff_str} {better}")
    
    print("-" * 70)
    print(f"{'ì²˜ë¦¬ ì‹œê°„':<20} {summary.v1_time_ms:>6.1f}ms{'':<12} {summary.v2_time_ms:>6.1f}ms")
    print(f"{'íƒì§€ ëŒ“ê¸€ ìˆ˜':<20} {summary.v1_flagged_count:>6}ê°œ{'':<12} {summary.v2_flagged_count:>6}ê°œ")
    
    # Confusion Matrix
    print(f"\n{'='*70}")
    print("ğŸ”¢ Confusion Matrix")
    print(f"{'='*70}")
    
    print("\n[V1 - 1í•™ê¸°]")
    print(f"              ì˜ˆì¸¡: ì •ìƒ    ì˜ˆì¸¡: ë…¼ë€")
    print(f"  ì‹¤ì œ ì •ìƒ:    {v1_metrics['tn']:>4}         {v1_metrics['fp']:>4}")
    print(f"  ì‹¤ì œ ë…¼ë€:    {v1_metrics['fn']:>4}         {v1_metrics['tp']:>4}")
    
    print("\n[V2 - 2í•™ê¸°]")
    print(f"              ì˜ˆì¸¡: ì •ìƒ    ì˜ˆì¸¡: ë…¼ë€")
    print(f"  ì‹¤ì œ ì •ìƒ:    {v2_metrics['tn']:>4}         {v2_metrics['fp']:>4}")
    print(f"  ì‹¤ì œ ë…¼ë€:    {v2_metrics['fn']:>4}         {v2_metrics['tp']:>4}")
    
    # ê²°ë¡ 
    print(f"\n{'='*70}")
    print("ğŸ“ ê²°ë¡ ")
    print(f"{'='*70}")
    
    if summary.v2_f1 > summary.v1_f1:
        improvement = (summary.v2_f1 - summary.v1_f1) * 100
        print(f"âœ… 2í•™ê¸° ë²„ì „ì´ F1 Score ê¸°ì¤€ {improvement:.1f}%p í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.")
    elif summary.v1_f1 > summary.v2_f1:
        decline = (summary.v1_f1 - summary.v2_f1) * 100
        print(f"âš ï¸ 2í•™ê¸° ë²„ì „ì´ F1 Score ê¸°ì¤€ {decline:.1f}%p í•˜ë½í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("â¡ï¸ ë‘ ë²„ì „ì˜ F1 Scoreê°€ ë™ì¼í•©ë‹ˆë‹¤.")
    
    print(f"\n[ë¼ë²¨ ë¹„êµ]")
    print(f"  V1: {V1_LABELS}")
    print(f"  V2: {V2_LABELS}")
    print(f"\n[ì„ê³„ê°’ ë¹„êµ]")
    print(f"  V1: ê°œë³„={V1_THRESHOLD}, ë¹„ìœ¨={V1_RATIO_THRESHOLD}")
    print(f"  V2: ê°œë³„={V2_THRESHOLD}, ë¹„ìœ¨={V2_RATIO_THRESHOLD}")


# ============================================================
# ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
# ============================================================
def analyze_by_category(results: List[CommentResult]):
    """ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„ (CATEGORY_INFO ì‚¬ìš©)"""
    
    print(f"\n{'='*70}")
    print("ğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ê²°ê³¼")
    print(f"{'='*70}")
    
    for cat_name, info in CATEGORY_INFO.items():
        start, end = info["range"]
        is_controversy_cat = info["expected"]
        
        cat_results = results[start:end]
        
        v1_correct = sum(1 for r in cat_results if r.v1_flagged == r.expected)
        v2_correct = sum(1 for r in cat_results if r.v2_flagged == r.expected)
        
        v1_flagged = sum(1 for r in cat_results if r.v1_flagged)
        v2_flagged = sum(1 for r in cat_results if r.v2_flagged)
        
        total = len(cat_results)
        
        print(f"\n[{cat_name}] ({total}ê°œ)")
        print(f"  ì •ë‹µ ë¼ë²¨: {'ğŸ”´ ë…¼ë€' if is_controversy_cat else 'ğŸŸ¢ ì •ìƒ'}")
        print(f"  V1 ì •í™•ë„: {v1_correct}/{total} ({v1_correct/total*100:.1f}%) | íƒì§€: {v1_flagged}ê°œ")
        print(f"  V2 ì •í™•ë„: {v2_correct}/{total} ({v2_correct/total*100:.1f}%) | íƒì§€: {v2_flagged}ê°œ")
        
        diff = v2_correct - v1_correct
        if diff > 0:
            print(f"  â†’ V2ê°€ {diff}ê°œ ë” ì •í™• â¬†ï¸")
        elif diff < 0:
            print(f"  â†’ V1ì´ {-diff}ê°œ ë” ì •í™• â¬‡ï¸")
        else:
            print(f"  â†’ ë™ì¼ â¡ï¸")


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================
if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("ğŸ”¬ ë…¼ë€ íƒì§€ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ: 1í•™ê¸° vs 2í•™ê¸°")
    print("=" * 70)
    print("ğŸ“ ë°ì´í„°: test_comments_100.py (100ê°œ ëŒ“ê¸€)")
    print("ğŸŒ ë²ˆì—­: DeepL API (í•œêµ­ì–´ â†’ ì˜ì–´)")
    
    # DeepL API í‚¤ í™•ì¸
    if DEEPL_API_KEY:
        print(f"âœ… DEEPL_API_KEY ë¡œë“œ ì™„ë£Œ")
    else:
        print(f"âš ï¸ DEEPL_API_KEY ì—†ìŒ - ì›ë¬¸ìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤")
    
    # í†µê³„ ì¶œë ¥
    stats = get_stats()
    print(f"\n[í…ŒìŠ¤íŠ¸ ë°ì´í„° êµ¬ì„±]")
    print(f"  ì´ ëŒ“ê¸€: {stats['total']}ê°œ")
    print(f"  â”œâ”€ ğŸ”´ ë…¼ë€ ëŒ“ê¸€ (ë’·ê´‘ê³ /í˜‘ì°¬): {stats['controversy']}ê°œ")
    print(f"  â””â”€ ğŸŸ¢ ì¼ë°˜ ëŒ“ê¸€: {stats['normal']}ê°œ")
    
    for cat_name, info in CATEGORY_INFO.items():
        start, end = info["range"]
        label = "ğŸ”´" if info["expected"] else "ğŸŸ¢"
        print(f"       â”œâ”€ {label} {cat_name}: {end - start}ê°œ")
    
    # ë¹„êµ ì‹¤í–‰
    results, summary, v1_metrics, v2_metrics = compare_versions(TEST_COMMENTS)
    
    # ê²°ê³¼ ì¶œë ¥
    print_results(results, summary, v1_metrics, v2_metrics)
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
    analyze_by_category(results)
    
    print("\n" + "=" * 70)
    print("âœ… ë¹„êµ ë¶„ì„ ì™„ë£Œ!")
    print("=" * 70 + "\n")