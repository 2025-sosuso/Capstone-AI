# src/pipelines/sentiment.py
from __future__ import annotations
import re
import asyncio
from collections import Counter
from typing import Dict, List, Tuple, Union # union ì¶”ê°€í•¨!!!!!!

import httpx
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

from src.config import DEEPL_API_KEY

_MNAME = "cardiffnlp/twitter-roberta-base-sentiment"

# ì „ì—­ ìºì‹œ(ì§€ì—° ë¡œë”©)
_tok = None
_model = None
_pipe = None

def _get_sentiment_pipeline():
    """
    ì²˜ìŒ í˜¸ì¶œë  ë•Œë§Œ ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ.
    - safetensorsë§Œ ì‚¬ìš© (torch 2.6 ë¯¸ë§Œì—ì„œë„ OK)
    - GPUê°€ ìˆìœ¼ë©´ ìë™ ì‚¬ìš©
    """
    global _tok, _model, _pipe
    if _pipe is None:
        _tok = AutoTokenizer.from_pretrained(_MNAME)
        _model = AutoModelForSequenceClassification.from_pretrained(
            _MNAME,
            use_safetensors=True,   # ğŸ”´ í•µì‹¬: .bin ëŒ€ì‹  safetensorsë§Œ ì‚¬ìš©
        )
        _pipe = pipeline(
            task="sentiment-analysis",
            model=_model,
            tokenizer=_tok,
            device=0 if torch.cuda.is_available() else -1,  # GPU ìë™ ì‚¬ìš©
            truncation=True,
            max_length=128,
            return_all_scores=False,  # ìµœê³  ì ìˆ˜ 1ê°œ ë¼ë²¨ë§Œ ë°˜í™˜
        )
    return _pipe


def _is_english(text: str) -> bool:
    cleaned = re.sub(r"[^\w\s.,!?\'\"-]", "", text)
    return bool(re.fullmatch(r"[A-Za-z0-9\s\.,;:'\"!?()\[\]{}@#$%^&*_\-=+/<>|~]+", cleaned))

async def _translate_text_async(client: httpx.AsyncClient, text: str, api_key: str, target_lang: str = "EN") -> str:
    """
    DeepL ë¹„ë™ê¸° ë²ˆì—­ (í‚¤ ì—†ìœ¼ë©´ ì›ë¬¸ ìœ ì§€)
    """
    if not api_key:
        return text
    url = "https://api.deepl.com/v2/translate"
    data = {"auth_key": api_key, "text": text, "target_lang": target_lang}
    try:
        resp = await client.post(url, data=data, timeout=15.0)
        resp.raise_for_status()
        return resp.json()["translations"][0]["text"]
    except Exception:
        return text

# !!!!!!!!!!!!!!
def _normalize_input(
    comments: Union[Dict[str, str], List[Tuple[str, str]], List[str]]
) -> Tuple[List[str], List[str]]:
    """
    ë‹¤ì–‘í•œ ì…ë ¥ì„ (ids, texts)ë¡œ ì •ê·œí™”
    - Dict[commentId, text]        -> (ids, texts)  âœ… commentId ìœ ì§€
    - List[Tuple[commentId, text]] -> (ids, texts)  âœ… commentId ìœ ì§€
    - List[str]                    -> (['0','1',...], texts)  (idê°€ ì—†ì„ ë•Œë§Œ ì¸ë±ìŠ¤)
    """
    if isinstance(comments, dict):
        ids, texts = zip(*comments.items()) if comments else ([], [])
        return list(ids), list(texts)

    if isinstance(comments, list) and comments and isinstance(comments[0], tuple):
        ids = [cid for cid, _t in comments]
        texts = [_t for _cid, _t in comments]
        return ids, texts

    # ê·¸ ì™¸(List[str] ë“±) â†’ ì¸ë±ìŠ¤ í‚¤ ì‚¬ìš©(í›„ë°©í˜¸í™˜)
    if isinstance(comments, list):
        texts = list(comments)
        ids = [str(i) for i in range(len(texts))]
        return ids, texts

    # fallback
    return [], []

async def analyze_sentiment_async(
    comments_dict: Union[Dict[str, str], List[Tuple[str, str]], List[str]]
) -> Tuple[Dict[str, str], Dict[str, float]]:
    """
    ì…ë ¥:
      - Dict[commentId, text]
      - List[Tuple[commentId, text]]
      - List[text] (ê¸°ì¡´ì²˜ëŸ¼ index ì‚¬ìš©)
    ì¶œë ¥:
      - (ê° ëŒ“ê¸€ì˜ ê°ì • ê²°ê³¼ Map, ì „ì²´ ë¹„ìœ¨ Map)
    """
    # âœ… ì…ë ¥ì„ (ids, texts)ë¡œ ì •ê·œí™”
    ids, texts = _normalize_input(comments_dict)
    if not texts:
        return {}, {}

    # âœ… ì˜ì–´ê°€ ì•„ë‹Œ ê²½ìš° DeepL ë²ˆì—­
    async with httpx.AsyncClient() as http_client:
        processed = [
            t if _is_english(t) else await _translate_text_async(http_client, t, DEEPL_API_KEY)
            for t in texts
        ]

    pipe = _get_sentiment_pipeline()
    results = pipe(list(processed), batch_size=64)

    # âœ… ê²°ê³¼ ë§¤í•‘
    label_map = {"LABEL_0": "NEGATIVE", "LABEL_1": "NEUTRAL", "LABEL_2": "POSITIVE"}
    per_comment: Dict[str, str] = {
        cid: label_map.get(r["label"], "NEUTRAL") for cid, r in zip(ids, results)
    }

    # âœ… ì „ì²´ ë¹„ìœ¨ ê³„ì‚°
    cnt = Counter(per_comment.values())
    total = max(sum(cnt.values()), 1)
    ratio = {
        "positive": round(cnt.get("POSITIVE", 0) / total * 100, 2),
        "neutral":  round(cnt.get("NEUTRAL", 0) / total * 100, 2),
        "negative": round(cnt.get("NEGATIVE", 0) / total * 100, 2),
    }

    # âœ… commentId ê¸°ì¤€ ê²°ê³¼ ë°˜í™˜
    return per_comment, ratio
