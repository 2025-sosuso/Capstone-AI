"""
ì¸ì½”ë” ì‚¬ì „ í•™ìŠµ ì—¬ë¶€ í™•ì¸ í…ŒìŠ¤íŠ¸
"""
from __future__ import annotations

import os
import sys
import warnings

if sys.platform == 'win32':
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    if hasattr(sys.stdout, 'reconfigure'):
        try:
            sys.stdout.reconfigure(encoding='utf-8')
        except Exception:
            pass

os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
warnings.filterwarnings("ignore", message=".*symlink.*")

from transformers import ElectraForSequenceClassification, ElectraModel
import torch

def check_encoder_pretrained():
    print("=" * 70)
    print("KoELECTRA-v3 ì¸ì½”ë” ì‚¬ì „ í•™ìŠµ ì—¬ë¶€ í™•ì¸")
    print("=" * 70)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # 1. GoEmotions ëª¨ë¸ ë¡œë“œ
    print("\n[1] monologg/koelectra-base-v3-goemotions ë¡œë“œ ì¤‘...")
    model_goemotions = ElectraForSequenceClassification.from_pretrained(
        "monologg/koelectra-base-v3-goemotions",
        use_safetensors=True
    ).to(device)
    
    # 2. ìˆœìˆ˜ KoELECTRA-v3-discriminator ë¡œë“œ (ë¹„êµìš©)
    print("[2] monologg/koelectra-base-v3-discriminator ë¡œë“œ ì¤‘...")
    model_pure = ElectraModel.from_pretrained(
        "monologg/koelectra-base-v3-discriminator"
    ).to(device)
    
    print("\n" + "=" * 70)
    print("íŒŒë¼ë¯¸í„° ë¶„ì„")
    print("=" * 70)
    
    # ë¡œë“œëœ íŒŒë¼ë¯¸í„° í™•ì¸
    goemotions_params = dict(model_goemotions.named_parameters())
    pure_params = dict(model_pure.named_parameters())
    
    print(f"\nì´ íŒŒë¼ë¯¸í„° ìˆ˜:")
    print(f"  - GoEmotions ëª¨ë¸: {len(goemotions_params):,}ê°œ")
    print(f"  - ìˆœìˆ˜ KoELECTRA: {len(pure_params):,}ê°œ")
    
    # ì¸ì½”ë” íŒŒë¼ë¯¸í„°ë§Œ í•„í„°ë§
    encoder_params_goemotions = {k: v for k, v in goemotions_params.items() 
                                  if k.startswith('electra.')}
    classifier_params = {k: v for k, v in goemotions_params.items() 
                        if k.startswith('classifier.')}
    
    print(f"\níŒŒë¼ë¯¸í„° êµ¬ì„±:")
    print(f"  - ì¸ì½”ë” íŒŒë¼ë¯¸í„°: {len(encoder_params_goemotions):,}ê°œ")
    print(f"  - Classifier íŒŒë¼ë¯¸í„°: {len(classifier_params):,}ê°œ")
    
    print(f"\nClassifier ë ˆì´ì–´ ëª©ë¡:")
    for name in classifier_params.keys():
        print(f"  - {name}")
    
    # ì¸ì½”ë” ê°€ì¤‘ì¹˜ í†µê³„ ë¶„ì„
    print("\n" + "=" * 70)
    print("ì¸ì½”ë” ê°€ì¤‘ì¹˜ í†µê³„ (ëœë¤ ì´ˆê¸°í™” vs ì‚¬ì „ í•™ìŠµ íŒë‹¨)")
    print("=" * 70)
    
    # ì²« ë²ˆì§¸ ì„ë² ë”© ë ˆì´ì–´ ë¶„ì„
    embedding_layer = "electra.embeddings.word_embeddings.weight"
    if embedding_layer in encoder_params_goemotions:
        weights = encoder_params_goemotions[embedding_layer]
        
        print(f"\n[{embedding_layer}]")
        print(f"  Shape: {weights.shape}")
        print(f"  Mean: {weights.mean().item():.6f}")
        print(f"  Std: {weights.std().item():.6f}")
        print(f"  Min: {weights.min().item():.6f}")
        print(f"  Max: {weights.max().item():.6f}")
        
        print(f"\n  ğŸ’¡ íŒë‹¨:")
        std = weights.std().item()
        if 0.01 < std < 0.15:  # ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë²”ìœ„
            print(f"     âœ… ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¡œ ë³´ì„ (Std: {std:.6f})")
            print(f"        (ëœë¤ ì´ˆê¸°í™”ë©´ Std ~0.02 ë˜ëŠ” ë§¤ìš° ì‘ì€ ê°’)")
        else:
            print(f"     âŒ ëœë¤ ì´ˆê¸°í™” ê°€ëŠ¥ì„± ìˆìŒ (Std: {std:.6f})")
    
    # ì²« ë²ˆì§¸ Transformer ë ˆì´ì–´ ë¶„ì„
    first_layer = "electra.encoder.layer.0.attention.self.query.weight"
    if first_layer in encoder_params_goemotions:
        weights = encoder_params_goemotions[first_layer]
        
        print(f"\n[{first_layer}]")
        print(f"  Shape: {weights.shape}")
        print(f"  Mean: {weights.mean().item():.6f}")
        print(f"  Std: {weights.std().item():.6f}")
        print(f"  Min: {weights.min().item():.6f}")
        print(f"  Max: {weights.max().item():.6f}")
        
        print(f"\n  ğŸ’¡ íŒë‹¨:")
        std = weights.std().item()
        if std > 0.05:  # ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ëŠ” ë” í° ë¶„ì‚°
            print(f"     âœ… ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¡œ ë³´ì„ (Std: {std:.6f})")
        else:
            print(f"     âŒ ëœë¤ ì´ˆê¸°í™” ê°€ëŠ¥ì„± ìˆìŒ (Std: {std:.6f})")
    
    # ì¸ì½”ë” ê°€ì¤‘ì¹˜ ë¹„êµ (ìˆœìˆ˜ ëª¨ë¸ê³¼)
    print("\n" + "=" * 70)
    print("ìˆœìˆ˜ KoELECTRAì™€ ê°€ì¤‘ì¹˜ ë¹„êµ")
    print("=" * 70)
    
    # ê³µí†µ íŒŒë¼ë¯¸í„° ì°¾ê¸°
    common_params = []
    for key_goe in encoder_params_goemotions.keys():
        # electra. ì œê±°í•˜ê³  ë¹„êµ
        key_pure = key_goe.replace("electra.", "")
        if key_pure in pure_params:
            common_params.append((key_goe, key_pure))
    
    print(f"\nê³µí†µ íŒŒë¼ë¯¸í„°: {len(common_params)}ê°œ")
    
    # ëª‡ ê°œ ìƒ˜í”Œë§í•´ì„œ ë¹„êµ
    sample_count = min(5, len(common_params))
    print(f"\nìƒ˜í”Œ {sample_count}ê°œ ë¹„êµ:")
    
    identical_count = 0
    for i, (key_goe, key_pure) in enumerate(common_params[:sample_count]):
        weights_goe = encoder_params_goemotions[key_goe]
        weights_pure = pure_params[key_pure]
        
        # ê°€ì¤‘ì¹˜ê°€ ë™ì¼í•œì§€ í™•ì¸ (í—ˆìš© ì˜¤ì°¨ 1e-6)
        is_identical = torch.allclose(weights_goe, weights_pure, atol=1e-6)
        
        print(f"\n  [{i+1}] {key_goe}")
        print(f"      ë™ì¼ ì—¬ë¶€: {'âœ… ë™ì¼' if is_identical else 'âŒ ë‹¤ë¦„'}")
        
        if is_identical:
            identical_count += 1
    
    print("\n" + "=" * 70)
    print("ìµœì¢… ê²°ë¡ ")
    print("=" * 70)
    
    if identical_count == sample_count:
        print("\nâœ… KoELECTRA-v3 ì¸ì½”ë”ëŠ” ì‚¬ì „ í•™ìŠµëœ ìƒíƒœì…ë‹ˆë‹¤!")
        print("   - ìˆœìˆ˜ KoELECTRA-v3 ëª¨ë¸ê³¼ ê°€ì¤‘ì¹˜ê°€ ë™ì¼í•¨")
        print("   - ê°€ì¤‘ì¹˜ ë¶„í¬ê°€ ì‚¬ì „ í•™ìŠµ íŒ¨í„´ì„ ë³´ì„")
        print("\nâœ… Classifierë§Œ ëœë¤ ì´ˆê¸°í™” ìƒíƒœì…ë‹ˆë‹¤.")
        print("   - Fine-tuningì´ í•„ìš”í•œ ë¶€ë¶„ì€ Classifierë§Œ")
    else:
        print(f"\nâš ï¸  ì¸ì½”ë” ê°€ì¤‘ì¹˜ í™•ì¸ í•„ìš”")
        print(f"   - {identical_count}/{sample_count}ê°œë§Œ ë™ì¼")

if __name__ == "__main__":
    check_encoder_pretrained()